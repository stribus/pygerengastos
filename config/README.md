# Configura√ß√µes de LLM

Este diret√≥rio cont√©m os arquivos de configura√ß√£o para os modelos de LLM do projeto.

## Arquivos

- `modelos_llm.toml`: Configura√ß√£o centralizada dos modelos dispon√≠veis (Gemini, LLaMA, Kimi, GPT-4o)

## Carregamento de Modelos

### Carregamento em Background

Os modelos s√£o carregados **automaticamente em background** durante a inicializa√ß√£o do Streamlit, garantindo que a interface fique dispon√≠vel rapidamente sem bloquear a UI.

**Caracter√≠sticas:**
- **Thread-safe**: Usa locks para garantir carregamento seguro em ambientes concorrentes
- **Cache em mem√≥ria**: Uma vez carregados, os modelos ficam em cache at√© o pr√≥ximo reload
- **Fallback autom√°tico**: Se o arquivo TOML estiver com erro ou ausente, usa configura√ß√£o hardcoded do Gemini
- **Lazy loading**: S√≥ carrega quando efetivamente necess√°rio

### Recarregar Configura√ß√µes

Voc√™ pode **recarregar as configura√ß√µes sem reiniciar a aplica√ß√£o**:

1. **Via UI**: Na p√°gina "Importar nota", clique no bot√£o "üîÑ Recarregar modelos" dentro de "‚öôÔ∏è Configura√ß√µes de LLM"
2. **Via c√≥digo**: Chame `recarregar_modelos()` de `src.classifiers.llm_classifier`

```python
from src.classifiers.llm_classifier import recarregar_modelos

# Invalida cache e recarrega do TOML
modelos_atualizados = recarregar_modelos()
```

## Tratamento de Erros

O sistema √© resiliente a erros de configura√ß√£o:

### Sintaxe TOML malformada
Se o arquivo TOML tiver erro de sintaxe, o sistema:
1. Loga o erro com detalhes (arquivo `logs/app.log`)
2. Retorna configura√ß√£o fallback (Gemini)
3. Continua funcionando sem interromper a aplica√ß√£o

### Campos obrigat√≥rios ausentes
Se um modelo n√£o tiver `nome` ou `api_key_env`:
1. O modelo inv√°lido √© **pulado**
2. Outros modelos v√°lidos s√£o carregados normalmente
3. Se **nenhum** modelo for v√°lido, usa fallback

### Arquivo inexistente
Se `config/modelos_llm.toml` n√£o existir:
1. Loga erro
2. Usa configura√ß√£o fallback (Gemini)

## Como adicionar um novo modelo

1. Abra `modelos_llm.toml`
2. Adicione um novo bloco `[[modelos]]` com a configura√ß√£o:

```toml
[[modelos]]
nome = "seu/modelo-id"
nome_amigavel = "Nome Amig√°vel para UI"
api_key_env = "SUA_API_KEY_ENV"
max_tokens = 4096
max_itens = 30
timeout = 30.0

# Opcional: extra_body simples se o modelo exigir par√¢metros espec√≠ficos
[modelos.extra_body]
custom_param = "value"

# Opcional: extra_body aninhado (ex.: Kimi K2.5)
[modelos.extra_body.chat_template_kwargs]
thinking = false
```

3. Certifique-se de que a vari√°vel de ambiente est√° configurada no `.env`
4. **Recarregue as configura√ß√µes** via UI ou c√≥digo (n√£o precisa reiniciar!)

## Seguran√ßa

As chaves de API **nunca** devem estar no arquivo TOML. Em vez disso, referencie o nome da vari√°vel de ambiente que ser√° carregada do `.env` em tempo de execu√ß√£o.

## Testes

O sistema possui testes abrangentes em `tests/test_llm_config_loading.py`:
- Carregamento de TOML v√°lido e inv√°lido
- Tratamento de campos obrigat√≥rios ausentes
- Carregamento concorrente thread-safe
- Cache e invalida√ß√£o
- Timeout e exce√ß√µes em background loading
- Fallback autom√°tico
